# Transformer
- Paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- Summary:
  - A new network architecture based on attention mechanism suggested.
  - Removes recurrence and convolution.
  - Reduces training time thanks to more parallelization.
  - Achieves 28.4 BLEU on the WMT 2014 English-to-German translation task and 41.8 BLEU on the WMT 2014 English-to-French translation task.

## References
- https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1
- https://pytorch.org/tutorials/beginner/transformer_tutorial.html
- https://nlp.seas.harvard.edu/2018/04/03/attention.html
- https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer.py
